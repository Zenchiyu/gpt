wandb:
  project: gpt
  mode: online

wandb_watch: True  # log more info

common:
  nb_steps: 6000  # nb of gradient steps
  sampling:
    context: O God, O God!   # prompt
    nb_tokens: 100  # nb of tokens to generate. TODO: K, V caching
    sampling_mode: prob  # prob, argmax
    temperature: 1
    save_path: ./results/txts/samples.txt  # relative to cwd
  training:
    chkpt_path: ./checkpoints/checkpoint.pth  # relative to cwd

dataset:
  path: ./data/shakespeare.txt
  chunk_size: ${model.max_seq_len}
  batch_size: 128
  num_workers: 0

model:
  max_seq_len: 128  # chunk size N
  embed_dim: 768
  mlp_hidden_dim: ${model.embed_dim}
  nb_layers: 12
  nb_heads: 8

optim:  # Adam optimizer
  lr: 1e-4
